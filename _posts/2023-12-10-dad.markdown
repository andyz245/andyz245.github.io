---
layout: post
title:  "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models"
date:   2023-12-01
image: /images/dad.png
categories: selected research
author: "Andy Zhou"
authors: "<strong>Andy Zhou</strong>, Jindong Wang, Haohan Wang, Yuxiong Wang"
venue: "NeurIPS"
arxiv: https://arxiv.org/abs/2311.01441 
code: https://github.com/andyz245/DiscreteAdversarialDistillation
slides: /pdfs/2311.01441.pdf
---
We propose a data augmentation and knowledge distillation objective that uses teacher gradients to generate diverse samples, improving out-of-distribution robustness. We distill from CLIP to train the most robust ResNet34 and ResNet50 on OOD generalization.