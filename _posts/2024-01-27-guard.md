---
layout: post
title:  "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models"
date:   2024-01-27
image: /images/guard.png
categories: research
author: "Andy Zhou"
authors: "Haibo Jin*, Ruoxi Chen*, <strong>Andy Zhou</strong>, Jinyin Chen, Yang Zhang, Haohan Wang"
venue: "arXiv"
arxiv: https://arxiv.org/abs/2402.03299
website: 
slides: /pdfs/guard.pdf
---
We propose a framework to generate semantic jailbreaks from human safety guidelines using syntatic parsing organized into knowledge graphs and LM optimization.
