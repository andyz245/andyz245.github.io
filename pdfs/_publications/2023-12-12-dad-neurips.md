---
title: "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models"
collection: publications
permalink: /publication/2023-12-10-dad-neurips
excerpt: 'We propose a data augmentation and knowledge distillation objective that uses teacher gradients to generate diverse samples, improving out-of-distribution robustness.'
date: 2023-12-10
venue: 'Neural Information Processing Systems'
paperurl: 'https://arxiv.org/abs/2311.01441'
---
We propose a data augmentation and knowledge distillation objective that uses teacher gradients to generate diverse samples, improving out-of-distribution robustness.

[Download paper here](http://andyz245.github.io/files/2311.1441.pdf)
