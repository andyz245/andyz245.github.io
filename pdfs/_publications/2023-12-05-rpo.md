---
title: "Robust Prompt Optimization for Defending Language Models"
collection: publications
permalink: /publication/2023-12-05-rpo
excerpt: 'We propose the first defense objective for defending LLMs against jailbreaking and an algorithm to generate trigger tokens that enforce harmless behavior, improving robustness across jailbreaks and models.'
date: 2023-12-04
venue: 'arXiv'
paperurl: ''
citation: ''
---
We formalize the first adversarial training objective for large language models which can defend against jailbreak attacks.

[Download paper here](http://andyz245.github.io/files/rpo.pdf)